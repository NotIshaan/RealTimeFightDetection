{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":397693,"sourceType":"datasetVersion","datasetId":176381}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow opencv-python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:39:12.771782Z","iopub.execute_input":"2025-05-09T15:39:12.772039Z","iopub.status.idle":"2025-05-09T15:39:16.764882Z","shell.execute_reply.started":"2025-05-09T15:39:12.772020Z","shell.execute_reply":"2025-05-09T15:39:16.763944Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.1)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.5.0)\nRequirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nimport os\nimport glob\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:39:19.368906Z","iopub.execute_input":"2025-05-09T15:39:19.369205Z","iopub.status.idle":"2025-05-09T15:39:34.493058Z","shell.execute_reply.started":"2025-05-09T15:39:19.369182Z","shell.execute_reply":"2025-05-09T15:39:34.492474Z"}},"outputs":[{"name":"stderr","text":"2025-05-09 15:39:21.230308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746805161.446801      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746805161.506972      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport cv2\nfrom tqdm import tqdm\n\n# Paths to violence and non-violence video folders\nviolence_dir = '/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence'\nnonviolence_dir = '/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence'\n\n# Output directories for frames\noutput_base = '/kaggle/working/frames'\nviolence_output = os.path.join(output_base, 'Violence')\nnonviolence_output = os.path.join(output_base, 'NonViolence')\n\n# Create output directories\nos.makedirs(violence_output, exist_ok=True)\nos.makedirs(nonviolence_output, exist_ok=True)\n\n# Function to extract N evenly spaced frames from a video\ndef extract_frames(video_path, output_folder, video_label, num_frames=5, resize_shape=(224, 224)):\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Get frame indices to extract\n    if total_frames < num_frames or total_frames == 0:\n        return  # Skip videos that are too short or corrupt\n    frame_indices = [int(i * total_frames / num_frames) for i in range(num_frames)]\n\n    frame_count = 0\n    saved = 0\n    success = True\n\n    while success and saved < num_frames:\n        success, frame = cap.read()\n        if frame_count in frame_indices:\n            frame = cv2.resize(frame, resize_shape)\n            video_name = os.path.basename(video_path).split('.')[0]\n            frame_filename = f\"{video_name}_frame_{saved}.jpg\"\n            frame_path = os.path.join(output_folder, frame_filename)\n            cv2.imwrite(frame_path, frame)\n            saved += 1\n        frame_count += 1\n\n    cap.release()\n\n# Process all videos\nprint(\"Extracting frames from Violence videos...\")\nfor video_file in tqdm(os.listdir(violence_dir)):\n    video_path = os.path.join(violence_dir, video_file)\n    extract_frames(video_path, violence_output, 'Violence')\n\nprint(\"Extracting frames from NonViolence videos...\")\nfor video_file in tqdm(os.listdir(nonviolence_dir)):\n    video_path = os.path.join(nonviolence_dir, video_file)\n    extract_frames(video_path, nonviolence_output, 'NonViolence')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:39:37.429912Z","iopub.execute_input":"2025-05-09T15:39:37.430487Z","iopub.status.idle":"2025-05-09T15:42:57.953959Z","shell.execute_reply.started":"2025-05-09T15:39:37.430461Z","shell.execute_reply":"2025-05-09T15:42:57.953135Z"}},"outputs":[{"name":"stdout","text":"Extracting frames from Violence videos...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [02:14<00:00,  7.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting frames from NonViolence videos...\n","output_type":"stream"},{"name":"stderr","text":" 50%|████▉     | 497/1000 [00:28<00:41, 12.10it/s][h264 @ 0x1d8992c0] mb_type 104 in P slice too large at 98 31\n[h264 @ 0x1d8992c0] error while decoding MB 98 31\n100%|██████████| 1000/1000 [01:06<00:00, 15.14it/s]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\n# Paths\nsource_dir = '/kaggle/working/frames'\ntrain_dir = '/kaggle/working/data/train'\nval_dir = '/kaggle/working/data/val'\n\n# Create the train and validation directories if they don't exist\nos.makedirs(train_dir, exist_ok=True)\nos.makedirs(val_dir, exist_ok=True)\n\n# Subdirectories for each class (Violence/NonViolence)\nfor subdir in ['Violence', 'NonViolence']:\n    os.makedirs(os.path.join(train_dir, subdir), exist_ok=True)\n    os.makedirs(os.path.join(val_dir, subdir), exist_ok=True)\n\n# Helper function to split files\ndef split_data(source_class_dir, train_class_dir, val_class_dir, test_size=0.2):\n    # Get all the files in the source directory\n    all_files = [f for f in os.listdir(source_class_dir) if os.path.isfile(os.path.join(source_class_dir, f))]\n    \n    # Split the files into train and validation sets\n    train_files, val_files = train_test_split(all_files, test_size=test_size, random_state=42)\n\n    # Move files to respective directories\n    for file in train_files:\n        shutil.copy(os.path.join(source_class_dir, file), os.path.join(train_class_dir, file))\n\n    for file in val_files:\n        shutil.copy(os.path.join(source_class_dir, file), os.path.join(val_class_dir, file))\n\n# Split data for each class\nsplit_data(os.path.join(source_dir, 'Violence'), os.path.join(train_dir, 'Violence'), os.path.join(val_dir, 'Violence'))\nsplit_data(os.path.join(source_dir, 'NonViolence'), os.path.join(train_dir, 'NonViolence'), os.path.join(val_dir, 'NonViolence'))\n\nprint(\"Data split into train and validation sets!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:03:56.880587Z","iopub.execute_input":"2025-05-09T16:03:56.880875Z","iopub.status.idle":"2025-05-09T16:03:57.997667Z","shell.execute_reply.started":"2025-05-09T16:03:56.880854Z","shell.execute_reply":"2025-05-09T16:03:57.996721Z"}},"outputs":[{"name":"stdout","text":"Data split into train and validation sets!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Image dimensions expected by InceptionV3\nimg_height, img_width = 299, 299\nbatch_size = 32\n\n# Create the training and validation data generators\ntrain_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True, rotation_range=20)\nval_datagen = ImageDataGenerator(rescale=1./255)\n\n# Load images from directory\ntrain_generator = train_datagen.flow_from_directory(\n    '/kaggle/working/data/train',\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\nval_generator = val_datagen.flow_from_directory(\n    '/kaggle/working/data/val',\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:05:28.962093Z","iopub.execute_input":"2025-05-09T16:05:28.962912Z","iopub.status.idle":"2025-05-09T16:05:29.055551Z","shell.execute_reply.started":"2025-05-09T16:05:28.962887Z","shell.execute_reply":"2025-05-09T16:05:29.055013Z"}},"outputs":[{"name":"stdout","text":"Found 8000 images belonging to 2 classes.\nFound 2000 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Load the InceptionV3 model without the top layer\nbase_model = InceptionV3(weights='imagenet', include_top=False)\n\n# Add custom classification layers\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation='relu')(x)\nx = Dense(1, activation='sigmoid')(x)  # Binary classification (violent/non-violent)\n\n# Create the final model\nmodel = Model(inputs=base_model.input, outputs=x)\n\n# Freeze the layers of the base model to prevent retraining them\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:04:18.472655Z","iopub.execute_input":"2025-05-09T16:04:18.473362Z","iopub.status.idle":"2025-05-09T16:04:20.597746Z","shell.execute_reply.started":"2025-05-09T16:04:18.473330Z","shell.execute_reply":"2025-05-09T16:04:20.597191Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=len(train_generator),\n    validation_data=val_generator,\n    validation_steps=len(val_generator),\n    epochs=5  # You can increase this to 10–15 if time allows\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:15:27.874698Z","iopub.execute_input":"2025-05-09T16:15:27.875209Z","iopub.status.idle":"2025-05-09T16:24:05.075576Z","shell.execute_reply.started":"2025-05-09T16:15:27.875179Z","shell.execute_reply":"2025-05-09T16:24:05.074959Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1746807345.066186    8082 service.cc:148] XLA service 0x7b8968025d40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1746807345.068781    8082 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1746807345.068805    8082 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1746807347.066956    8082 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  1/250\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:01:03\u001b[0m 29s/step - accuracy: 0.4688 - loss: 0.7750","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1746807358.185892    8082 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 651ms/step - accuracy: 0.7923 - loss: 0.4223 - val_accuracy: 0.8609 - val_loss: 0.3130\nEpoch 2/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9375 - val_loss: 0.2028\nEpoch 3/5\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 620ms/step - accuracy: 0.9010 - loss: 0.2380 - val_accuracy: 0.8876 - val_loss: 0.2524\nEpoch 4/5\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9375 - val_loss: 0.0954\nEpoch 5/5\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 614ms/step - accuracy: 0.9152 - loss: 0.2052 - val_accuracy: 0.8972 - val_loss: 0.2608\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"model.save('violence_detection_model.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:26:16.214581Z","iopub.execute_input":"2025-05-09T16:26:16.215368Z","iopub.status.idle":"2025-05-09T16:26:16.809974Z","shell.execute_reply.started":"2025-05-09T16:26:16.215343Z","shell.execute_reply":"2025-05-09T16:26:16.809189Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras.preprocessing import image\n\ndef extract_frames_from_video(video_path, num_frames=16, target_size=(224, 224)):\n    frames = []\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    if total_frames < num_frames:\n        frame_indices = np.linspace(0, total_frames - 1, total_frames).astype(int)\n    else:\n        frame_indices = np.linspace(0, total_frames - 1, num_frames).astype(int)\n\n    count = 0\n    grabbed = 0\n    while True:\n        success, frame = cap.read()\n        if not success:\n            break\n        if count in frame_indices:\n            frame = cv2.resize(frame, target_size)\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame)\n            grabbed += 1\n            if grabbed == len(frame_indices):\n                break\n        count += 1\n    cap.release()\n    \n    return np.array(frames)\n\nvideo_path = \"/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence/NV_126.mp4\"\nframes = extract_frames_from_video(video_path, num_frames=16)\nframes = frames / 255.0\n\n# Predict on each frame, then average\npreds = model.predict(frames)\navg_pred = np.mean(preds)\n\nlabel = \"Violent\" if avg_pred > 0.5 else \"Non-Violent\"\nprint(f\"Prediction: {label} (confidence: {avg_pred:.2f})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:32:10.692108Z","iopub.execute_input":"2025-05-09T16:32:10.692403Z","iopub.status.idle":"2025-05-09T16:32:10.884838Z","shell.execute_reply.started":"2025-05-09T16:32:10.692383Z","shell.execute_reply":"2025-05-09T16:32:10.884277Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\nPrediction: Non-Violent (confidence: 0.16)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Folder paths\nnon_violent_folder = \"/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence\"\nviolent_folder = \"/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence\"\n\n# Limit to 100 total videos (50 from each category)\nnon_violent_videos = sorted([os.path.join(non_violent_folder, f) for f in os.listdir(non_violent_folder) if f.endswith(\".mp4\")])[:50]\nviolent_videos = sorted([os.path.join(violent_folder, f) for f in os.listdir(violent_folder) if f.endswith(\".mp4\")])[:50]\n\nvideo_paths = [(path, 0) for path in non_violent_videos] + [(path, 1) for path in violent_videos]\n\ndef extract_frames(video_path, num_frames=5):\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    step = max(1, total_frames // num_frames)\n    \n    frames = []\n    for i in range(0, total_frames, step):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n        ret, frame = cap.read()\n        if ret:\n            frame = cv2.resize(frame, (224, 224))\n            frames.append(frame)\n        if len(frames) >= num_frames:\n            break\n    cap.release()\n    return np.array(frames)\n\ndef predict_video(video_path, model):\n    frames = extract_frames(video_path)\n    frames = frames.astype('float32') / 255.0\n    preds = model.predict(frames, verbose=0)\n    avg_pred = np.mean(preds)\n    return 1 if avg_pred > 0.5 else 0\n\n# Evaluate\ny_true = []\ny_pred = []\n\nprint(\"Evaluating on 100 videos...\")\nfor path, label in tqdm(video_paths):\n    pred = predict_video(path, model)\n    y_true.append(label)\n    y_pred.append(pred)\n\n# Metrics\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred, target_names=[\"Non-Violent\", \"Violent\"]))\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_true, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:36:49.312251Z","iopub.execute_input":"2025-05-09T16:36:49.312634Z","iopub.status.idle":"2025-05-09T16:37:15.511314Z","shell.execute_reply.started":"2025-05-09T16:36:49.312613Z","shell.execute_reply":"2025-05-09T16:37:15.510419Z"}},"outputs":[{"name":"stdout","text":"Evaluating on 100 videos...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:26<00:00,  3.82it/s]","output_type":"stream"},{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n Non-Violent       1.00      0.70      0.82        50\n     Violent       0.77      1.00      0.87        50\n\n    accuracy                           0.85       100\n   macro avg       0.88      0.85      0.85       100\nweighted avg       0.88      0.85      0.85       100\n\nConfusion Matrix:\n[[35 15]\n [ 0 50]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}