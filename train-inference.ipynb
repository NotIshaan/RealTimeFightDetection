{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":397693,"sourceType":"datasetVersion","datasetId":176381},{"sourceId":11756855,"sourceType":"datasetVersion","datasetId":7380702},{"sourceId":11756877,"sourceType":"datasetVersion","datasetId":7380711},{"sourceId":11757520,"sourceType":"datasetVersion","datasetId":7381081},{"sourceId":11757577,"sourceType":"datasetVersion","datasetId":7381118}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow opencv-python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:22:46.418574Z","iopub.execute_input":"2025-05-10T06:22:46.418807Z","iopub.status.idle":"2025-05-10T06:22:50.839911Z","shell.execute_reply.started":"2025-05-10T06:22:46.418782Z","shell.execute_reply":"2025-05-10T06:22:50.839002Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.1)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.5.0)\nRequirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nimport os\nimport glob\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:22:50.842060Z","iopub.execute_input":"2025-05-10T06:22:50.842517Z","iopub.status.idle":"2025-05-10T06:23:04.229918Z","shell.execute_reply.started":"2025-05-10T06:22:50.842485Z","shell.execute_reply":"2025-05-10T06:23:04.229330Z"}},"outputs":[{"name":"stderr","text":"2025-05-10 06:22:52.944034: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746858173.146916      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746858173.206171      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport cv2\nfrom tqdm import tqdm\n\n# Paths to violence and non-violence video folders\nviolence_dir = '/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence'\nnonviolence_dir = '/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence'\n\n# Output directories for frames\noutput_base = '/kaggle/working/frames'\nviolence_output = os.path.join(output_base, 'Violence')\nnonviolence_output = os.path.join(output_base, 'NonViolence')\n\n# Create output directories\nos.makedirs(violence_output, exist_ok=True)\nos.makedirs(nonviolence_output, exist_ok=True)\n\n# Function to extract N evenly spaced frames from a video\ndef extract_frames(video_path, output_folder, video_label, num_frames=15, resize_shape=(299, 299)):\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Get frame indices to extract\n    if total_frames < num_frames or total_frames == 0:\n        return  # Skip videos that are too short or corrupt\n    frame_indices = [int(i * total_frames / num_frames) for i in range(num_frames)]\n\n    frame_count = 0\n    saved = 0\n    success = True\n\n    while success and saved < num_frames:\n        success, frame = cap.read()\n        if frame_count in frame_indices:\n            frame = cv2.resize(frame, resize_shape)\n            video_name = os.path.basename(video_path).split('.')[0]\n            frame_filename = f\"{video_name}_frame_{saved}.jpg\"\n            frame_path = os.path.join(output_folder, frame_filename)\n            cv2.imwrite(frame_path, frame)\n            saved += 1\n        frame_count += 1\n\n    cap.release()\n\n# Process all videos\nprint(\"Extracting frames from Violence videos...\")\nfor video_file in tqdm(os.listdir(violence_dir)):\n    video_path = os.path.join(violence_dir, video_file)\n    extract_frames(video_path, violence_output, 'Violence', num_frames=15)\n\nprint(\"Extracting frames from NonViolence videos...\")\nfor video_file in tqdm(os.listdir(nonviolence_dir)):\n    video_path = os.path.join(nonviolence_dir, video_file)\n    extract_frames(video_path, nonviolence_output, 'NonViolence', num_frames=15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:23:04.230683Z","iopub.execute_input":"2025-05-10T06:23:04.231122Z","iopub.status.idle":"2025-05-10T06:27:10.276026Z","shell.execute_reply.started":"2025-05-10T06:23:04.231103Z","shell.execute_reply":"2025-05-10T06:27:10.275442Z"}},"outputs":[{"name":"stdout","text":"Extracting frames from Violence videos...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [02:41<00:00,  6.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting frames from NonViolence videos...\n","output_type":"stream"},{"name":"stderr","text":" 50%|████▉     | 496/1000 [00:37<00:50,  9.93it/s][h264 @ 0x1411b040] mb_type 104 in P slice too large at 98 31\n[h264 @ 0x1411b040] error while decoding MB 98 31\n100%|██████████| 1000/1000 [01:24<00:00, 11.90it/s]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\n# Paths\nsource_dir = '/kaggle/working/frames'\ntrain_dir = '/kaggle/working/data/train'\nval_dir = '/kaggle/working/data/val'\n\n# Create the train and validation directories if they don't exist\nos.makedirs(train_dir, exist_ok=True)\nos.makedirs(val_dir, exist_ok=True)\n\n# Subdirectories for each class (Violence/NonViolence)\nfor subdir in ['Violence', 'NonViolence']:\n    os.makedirs(os.path.join(train_dir, subdir), exist_ok=True)\n    os.makedirs(os.path.join(val_dir, subdir), exist_ok=True)\n\n# Helper function to split files\ndef split_data(source_class_dir, train_class_dir, val_class_dir, test_size=0.2):\n    # Get all the files in the source directory\n    all_files = [f for f in os.listdir(source_class_dir) if os.path.isfile(os.path.join(source_class_dir, f))]\n    \n    # Split the files into train and validation sets\n    train_files, val_files = train_test_split(all_files, test_size=test_size, random_state=42)\n\n    # Move files to respective directories\n    for file in train_files:\n        shutil.copy(os.path.join(source_class_dir, file), os.path.join(train_class_dir, file))\n\n    for file in val_files:\n        shutil.copy(os.path.join(source_class_dir, file), os.path.join(val_class_dir, file))\n\n# Split data for each class\nsplit_data(os.path.join(source_dir, 'Violence'), os.path.join(train_dir, 'Violence'), os.path.join(val_dir, 'Violence'))\nsplit_data(os.path.join(source_dir, 'NonViolence'), os.path.join(train_dir, 'NonViolence'), os.path.join(val_dir, 'NonViolence'))\n\nprint(\"Data split into train and validation sets!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:40:16.517974Z","iopub.execute_input":"2025-05-10T06:40:16.518261Z","iopub.status.idle":"2025-05-10T06:40:21.260237Z","shell.execute_reply.started":"2025-05-10T06:40:16.518233Z","shell.execute_reply":"2025-05-10T06:40:21.259586Z"}},"outputs":[{"name":"stdout","text":"Data split into train and validation sets!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Image dimensions expected by InceptionV3\nimg_height, img_width = 299, 299\nbatch_size = 32\n\n# Create the training and validation data generators\ntrain_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True, rotation_range=20)\nval_datagen = ImageDataGenerator(rescale=1./255)\n\n# Load images from directory\ntrain_generator = train_datagen.flow_from_directory(\n    '/kaggle/working/data/train',\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\nval_generator = val_datagen.flow_from_directory(\n    '/kaggle/working/data/val',\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:40:36.742811Z","iopub.execute_input":"2025-05-10T06:40:36.743522Z","iopub.status.idle":"2025-05-10T06:40:36.986401Z","shell.execute_reply.started":"2025-05-10T06:40:36.743497Z","shell.execute_reply":"2025-05-10T06:40:36.985866Z"}},"outputs":[{"name":"stdout","text":"Found 24000 images belonging to 2 classes.\nFound 6000 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Loading the Model","metadata":{}},{"cell_type":"code","source":"# Load the InceptionV3 model without the top layer\nbase_model = InceptionV3(weights='imagenet', include_top=False)\n\n# Add custom classification layers\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation='relu')(x)\nx = Dense(1, activation='sigmoid')(x)  # Binary classification (violent/non-violent)\n\n# Create the final model\nmodel = Model(inputs=base_model.input, outputs=x)\n\n# Freeze the layers of the base model to prevent retraining them\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:40:41.258209Z","iopub.execute_input":"2025-05-10T06:40:41.258769Z","iopub.status.idle":"2025-05-10T06:40:42.769797Z","shell.execute_reply.started":"2025-05-10T06:40:41.258746Z","shell.execute_reply":"2025-05-10T06:40:42.769209Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# MODEL TRAINING AND SAVING","metadata":{}},{"cell_type":"code","source":"# Train the model with a higher number of epochs\nhistory = model.fit(\n    train_generator,\n    validation_data=val_generator,\n    epochs=15,  # Suggest 15 epochs for better convergence\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n        tf.keras.callbacks.ModelCheckpoint('VDM_v2.keras', monitor='val_loss', save_best_only=True)\n    ]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:40:43.152583Z","iopub.execute_input":"2025-05-10T06:40:43.152815Z","iopub.status.idle":"2025-05-10T08:33:38.939637Z","shell.execute_reply.started":"2025-05-10T06:40:43.152800Z","shell.execute_reply":"2025-05-10T08:33:38.939019Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/15\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 614ms/step - accuracy: 0.8614 - loss: 0.3156 - val_accuracy: 0.9320 - val_loss: 0.1716\nEpoch 2/15\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 599ms/step - accuracy: 0.9342 - loss: 0.1666 - val_accuracy: 0.9475 - val_loss: 0.1311\nEpoch 3/15\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 599ms/step - accuracy: 0.9525 - loss: 0.1301 - val_accuracy: 0.9573 - val_loss: 0.1121\nEpoch 4/15\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m450s\u001b[0m 595ms/step - accuracy: 0.9625 - loss: 0.1040 - val_accuracy: 0.9537 - val_loss: 0.1162\nEpoch 5/15\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 591ms/step - accuracy: 0.9689 - loss: 0.0929 - val_accuracy: 0.9652 - val_loss: 0.0911\nEpoch 6/15\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 593ms/step - accuracy: 0.9691 - loss: 0.0828 - val_accuracy: 0.9647 - val_loss: 0.0859\nEpoch 7/15\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 593ms/step - accuracy: 0.9745 - loss: 0.0703 - val_accuracy: 0.9685 - val_loss: 0.0810\nEpoch 8/15\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 592ms/step - accuracy: 0.9798 - loss: 0.0612 - val_accuracy: 0.9657 - val_loss: 0.0903\nEpoch 9/15\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 594ms/step - accuracy: 0.9800 - loss: 0.0583 - val_accuracy: 0.9742 - val_loss: 0.0664\nEpoch 10/15\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 591ms/step - accuracy: 0.9815 - loss: 0.0548 - val_accuracy: 0.9740 - val_loss: 0.0684\nEpoch 11/15\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 593ms/step - accuracy: 0.9858 - loss: 0.0430 - val_accuracy: 0.9737 - val_loss: 0.0686\nEpoch 12/15\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m450s\u001b[0m 594ms/step - accuracy: 0.9849 - loss: 0.0425 - val_accuracy: 0.9782 - val_loss: 0.0571\nEpoch 13/15\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 594ms/step - accuracy: 0.9885 - loss: 0.0352 - val_accuracy: 0.9768 - val_loss: 0.0633\nEpoch 14/15\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m451s\u001b[0m 595ms/step - accuracy: 0.9859 - loss: 0.0414 - val_accuracy: 0.9802 - val_loss: 0.0532\nEpoch 15/15\n\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m452s\u001b[0m 597ms/step - accuracy: 0.9891 - loss: 0.0336 - val_accuracy: 0.9815 - val_loss: 0.0515\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"model.save('violence_detection_model_v2.keras')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T09:40:35.566138Z","iopub.execute_input":"2025-05-10T09:40:35.566715Z","iopub.status.idle":"2025-05-10T09:40:35.581507Z","shell.execute_reply.started":"2025-05-10T09:40:35.566690Z","shell.execute_reply":"2025-05-10T09:40:35.580592Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/4289644244.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'violence_detection_model_v2.keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"markdown","source":"# INFERENCE VIDEO FRAME EXTRACTION","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load the saved model\nmodel = load_model('/kaggle/working/VDM_v2.keras')  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T09:42:14.726356Z","iopub.execute_input":"2025-05-10T09:42:14.727151Z","iopub.status.idle":"2025-05-10T09:42:22.688370Z","shell.execute_reply.started":"2025-05-10T09:42:14.727124Z","shell.execute_reply":"2025-05-10T09:42:22.687786Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1746870138.217970      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1746870138.218640      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras.preprocessing import image\n\ndef extract_frames_from_video(video_path, num_frames=16, target_size=(299, 299)):\n    frames = []\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    if total_frames < num_frames:\n        frame_indices = np.linspace(0, total_frames - 1, total_frames).astype(int)\n    else:\n        frame_indices = np.linspace(0, total_frames - 1, num_frames).astype(int)\n\n    count = 0\n    grabbed = 0\n    while True:\n        success, frame = cap.read()\n        if not success:\n            break\n        if count in frame_indices:\n            frame = cv2.resize(frame, target_size)\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame)\n            grabbed += 1\n            if grabbed == len(frame_indices):\n                break\n        count += 1\n    cap.release()\n    \n    return np.array(frames)\n\nvideo_path = \"/kaggle/input/1080ptestviolence/1080p.mp4\"\nframes = extract_frames_from_video(video_path, num_frames=32)\nframes = preprocess_input(frames.astype(np.float32))\n\n# Predict on each frame, then average\npreds = model.predict(frames)\navg_pred = np.mean(preds)\n\nlabel = \"Violent\" if avg_pred > 0.5 else \"Non-Violent\"\nprint(f\"Prediction: {label} (confidence: {avg_pred:.2f})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T09:51:23.541550Z","iopub.execute_input":"2025-05-10T09:51:23.542282Z","iopub.status.idle":"2025-05-10T09:51:26.962922Z","shell.execute_reply.started":"2025-05-10T09:51:23.542258Z","shell.execute_reply":"2025-05-10T09:51:26.962280Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\nPrediction: Violent (confidence: 0.62)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Folder paths\nnon_violent_folder = \"/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence\"\nviolent_folder = \"/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence\"\n\n# Limit to 100 total videos (50 from each category)\nnon_violent_videos = sorted([os.path.join(non_violent_folder, f) for f in os.listdir(non_violent_folder) if f.endswith(\".mp4\")])[:50]\nviolent_videos = sorted([os.path.join(violent_folder, f) for f in os.listdir(violent_folder) if f.endswith(\".mp4\")])[:50]\n\nvideo_paths = [(path, 0) for path in non_violent_videos] + [(path, 1) for path in violent_videos]\n\ndef extract_frames(video_path, num_frames=5):\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    step = max(1, total_frames // num_frames)\n    \n    frames = []\n    for i in range(0, total_frames, step):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n        ret, frame = cap.read()\n        if ret:\n            frame = cv2.resize(frame, (224, 224))\n            frames.append(frame)\n        if len(frames) >= num_frames:\n            break\n    cap.release()\n    return np.array(frames)\n\ndef predict_video(video_path, model):\n    frames = extract_frames(video_path)\n    frames = frames.astype('float32') / 255.0\n    preds = model.predict(frames, verbose=0)\n    avg_pred = np.mean(preds)\n    return 1 if avg_pred > 0.5 else 0\n\n# Evaluate\ny_true = []\ny_pred = []\n\nprint(\"Evaluating on 100 videos...\")\nfor path, label in tqdm(video_paths):\n    pred = predict_video(path, model)\n    y_true.append(label)\n    y_pred.append(pred)\n\n# Metrics\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred, target_names=[\"Non-Violent\", \"Violent\"]))\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_true, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T08:37:14.128825Z","iopub.execute_input":"2025-05-10T08:37:14.129088Z","iopub.status.idle":"2025-05-10T08:37:48.076827Z","shell.execute_reply.started":"2025-05-10T08:37:14.129066Z","shell.execute_reply":"2025-05-10T08:37:48.076080Z"}},"outputs":[{"name":"stdout","text":"Evaluating on 100 videos...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:33<00:00,  2.95it/s]","output_type":"stream"},{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n Non-Violent       0.98      0.96      0.97        50\n     Violent       0.96      0.98      0.97        50\n\n    accuracy                           0.97       100\n   macro avg       0.97      0.97      0.97       100\nweighted avg       0.97      0.97      0.97       100\n\nConfusion Matrix:\n[[48  2]\n [ 1 49]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}