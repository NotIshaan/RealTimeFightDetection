{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d85b51fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T18:52:35.610108Z",
     "iopub.status.busy": "2025-05-09T18:52:35.609407Z",
     "iopub.status.idle": "2025-05-09T18:52:39.505711Z",
     "shell.execute_reply": "2025-05-09T18:52:39.504980Z"
    },
    "papermill": {
     "duration": 3.90162,
     "end_time": "2025-05-09T18:52:39.507563",
     "exception": false,
     "start_time": "2025-05-09T18:52:35.605943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\r\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.1)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\r\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\r\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.5.0)\r\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\r\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\r\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\r\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\r\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\r\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c7b7e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T18:52:39.513759Z",
     "iopub.status.busy": "2025-05-09T18:52:39.513256Z",
     "iopub.status.idle": "2025-05-09T18:52:53.401329Z",
     "shell.execute_reply": "2025-05-09T18:52:53.400552Z"
    },
    "papermill": {
     "duration": 13.892542,
     "end_time": "2025-05-09T18:52:53.402835",
     "exception": false,
     "start_time": "2025-05-09T18:52:39.510293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 18:52:41.197729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746816761.404171      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746816761.467554      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306f06f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T18:52:53.408712Z",
     "iopub.status.busy": "2025-05-09T18:52:53.408273Z",
     "iopub.status.idle": "2025-05-09T18:56:48.098982Z",
     "shell.execute_reply": "2025-05-09T18:56:48.098196Z"
    },
    "papermill": {
     "duration": 234.694919,
     "end_time": "2025-05-09T18:56:48.100272",
     "exception": false,
     "start_time": "2025-05-09T18:52:53.405353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames from Violence videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:34<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames from NonViolence videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 497/1000 [00:35<01:09,  7.21it/s][h264 @ 0xb1d4fc0] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0xb1d4fc0] error while decoding MB 98 31\n",
      "100%|██████████| 1000/1000 [01:20<00:00, 12.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths to violence and non-violence video folders\n",
    "violence_dir = '/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence'\n",
    "nonviolence_dir = '/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence'\n",
    "\n",
    "# Output directories for frames\n",
    "output_base = '/kaggle/working/frames'\n",
    "violence_output = os.path.join(output_base, 'Violence')\n",
    "nonviolence_output = os.path.join(output_base, 'NonViolence')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(violence_output, exist_ok=True)\n",
    "os.makedirs(nonviolence_output, exist_ok=True)\n",
    "\n",
    "# Function to extract N evenly spaced frames from a video\n",
    "def extract_frames(video_path, output_folder, video_label, num_frames=15, resize_shape=(299, 299)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Get frame indices to extract\n",
    "    if total_frames < num_frames or total_frames == 0:\n",
    "        return  # Skip videos that are too short or corrupt\n",
    "    frame_indices = [int(i * total_frames / num_frames) for i in range(num_frames)]\n",
    "\n",
    "    frame_count = 0\n",
    "    saved = 0\n",
    "    success = True\n",
    "\n",
    "    while success and saved < num_frames:\n",
    "        success, frame = cap.read()\n",
    "        if frame_count in frame_indices:\n",
    "            frame = cv2.resize(frame, resize_shape)\n",
    "            video_name = os.path.basename(video_path).split('.')[0]\n",
    "            frame_filename = f\"{video_name}_frame_{saved}.jpg\"\n",
    "            frame_path = os.path.join(output_folder, frame_filename)\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            saved += 1\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "# Process all videos\n",
    "print(\"Extracting frames from Violence videos...\")\n",
    "for video_file in tqdm(os.listdir(violence_dir)):\n",
    "    video_path = os.path.join(violence_dir, video_file)\n",
    "    extract_frames(video_path, violence_output, 'Violence', num_frames=15)\n",
    "\n",
    "print(\"Extracting frames from NonViolence videos...\")\n",
    "for video_file in tqdm(os.listdir(nonviolence_dir)):\n",
    "    video_path = os.path.join(nonviolence_dir, video_file)\n",
    "    extract_frames(video_path, nonviolence_output, 'NonViolence', num_frames=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27bf74fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T18:56:48.219426Z",
     "iopub.status.busy": "2025-05-09T18:56:48.218693Z",
     "iopub.status.idle": "2025-05-09T18:56:52.810342Z",
     "shell.execute_reply": "2025-05-09T18:56:52.809412Z"
    },
    "papermill": {
     "duration": 4.630115,
     "end_time": "2025-05-09T18:56:52.811756",
     "exception": false,
     "start_time": "2025-05-09T18:56:48.181641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into train and validation sets!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths\n",
    "source_dir = '/kaggle/working/frames'\n",
    "train_dir = '/kaggle/working/data/train'\n",
    "val_dir = '/kaggle/working/data/val'\n",
    "\n",
    "# Create the train and validation directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Subdirectories for each class (Violence/NonViolence)\n",
    "for subdir in ['Violence', 'NonViolence']:\n",
    "    os.makedirs(os.path.join(train_dir, subdir), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_dir, subdir), exist_ok=True)\n",
    "\n",
    "# Helper function to split files\n",
    "def split_data(source_class_dir, train_class_dir, val_class_dir, test_size=0.2):\n",
    "    # Get all the files in the source directory\n",
    "    all_files = [f for f in os.listdir(source_class_dir) if os.path.isfile(os.path.join(source_class_dir, f))]\n",
    "    \n",
    "    # Split the files into train and validation sets\n",
    "    train_files, val_files = train_test_split(all_files, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Move files to respective directories\n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(source_class_dir, file), os.path.join(train_class_dir, file))\n",
    "\n",
    "    for file in val_files:\n",
    "        shutil.copy(os.path.join(source_class_dir, file), os.path.join(val_class_dir, file))\n",
    "\n",
    "# Split data for each class\n",
    "split_data(os.path.join(source_dir, 'Violence'), os.path.join(train_dir, 'Violence'), os.path.join(val_dir, 'Violence'))\n",
    "split_data(os.path.join(source_dir, 'NonViolence'), os.path.join(train_dir, 'NonViolence'), os.path.join(val_dir, 'NonViolence'))\n",
    "\n",
    "print(\"Data split into train and validation sets!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "885fb28c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T18:56:52.886806Z",
     "iopub.status.busy": "2025-05-09T18:56:52.886340Z",
     "iopub.status.idle": "2025-05-09T18:56:53.130117Z",
     "shell.execute_reply": "2025-05-09T18:56:53.129581Z"
    },
    "papermill": {
     "duration": 0.281869,
     "end_time": "2025-05-09T18:56:53.131238",
     "exception": false,
     "start_time": "2025-05-09T18:56:52.849369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24000 images belonging to 2 classes.\n",
      "Found 6000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Image dimensions expected by InceptionV3\n",
    "img_height, img_width = 299, 299\n",
    "batch_size = 32\n",
    "\n",
    "# Create the training and validation data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True, rotation_range=20)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load images from directory\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/kaggle/working/data/train',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    '/kaggle/working/data/val',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a77ca61a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T18:56:53.206349Z",
     "iopub.status.busy": "2025-05-09T18:56:53.206124Z",
     "iopub.status.idle": "2025-05-09T18:56:57.718609Z",
     "shell.execute_reply": "2025-05-09T18:56:57.718028Z"
    },
    "papermill": {
     "duration": 4.550865,
     "end_time": "2025-05-09T18:56:57.719871",
     "exception": false,
     "start_time": "2025-05-09T18:56:53.169006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746817014.357412      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1746817014.358099      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the InceptionV3 model without the top layer\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# Add custom classification layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(1, activation='sigmoid')(x)  # Binary classification (violent/non-violent)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Freeze the layers of the base model to prevent retraining them\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31067724",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T18:56:57.794788Z",
     "iopub.status.busy": "2025-05-09T18:56:57.794579Z",
     "iopub.status.idle": "2025-05-09T19:59:42.337244Z",
     "shell.execute_reply": "2025-05-09T19:59:42.336437Z"
    },
    "papermill": {
     "duration": 3764.738094,
     "end_time": "2025-05-09T19:59:42.495267",
     "exception": false,
     "start_time": "2025-05-09T18:56:57.757173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746817034.566988    8081 service.cc:148] XLA service 0x7e368005a030 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1746817034.567647    8081 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1746817034.567671    8081 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1746817036.516546    8081 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/750\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:31:34\u001b[0m 27s/step - accuracy: 0.5312 - loss: 0.7230"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746817045.417363    8081 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 633ms/step - accuracy: 0.8645 - loss: 0.3085 - val_accuracy: 0.9293 - val_loss: 0.1720\n",
      "Epoch 2/15\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "  current = self.get_monitor_value(logs)\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_loss available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 618ms/step - accuracy: 0.9375 - loss: 0.1652 - val_accuracy: 0.9493 - val_loss: 0.1369\n",
      "Epoch 4/15\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 5/15\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 618ms/step - accuracy: 0.9545 - loss: 0.1294 - val_accuracy: 0.9485 - val_loss: 0.1332\n",
      "Epoch 6/15\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/15\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m464s\u001b[0m 613ms/step - accuracy: 0.9616 - loss: 0.1064 - val_accuracy: 0.9550 - val_loss: 0.1176\n",
      "Epoch 8/15\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 9/15\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 616ms/step - accuracy: 0.9630 - loss: 0.1024 - val_accuracy: 0.9650 - val_loss: 0.0969\n",
      "Epoch 10/15\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 11/15\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 617ms/step - accuracy: 0.9709 - loss: 0.0795 - val_accuracy: 0.9658 - val_loss: 0.0905\n",
      "Epoch 12/15\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 13/15\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 617ms/step - accuracy: 0.9755 - loss: 0.0712 - val_accuracy: 0.9705 - val_loss: 0.0792\n",
      "Epoch 14/15\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 15/15\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m463s\u001b[0m 612ms/step - accuracy: 0.9798 - loss: 0.0577 - val_accuracy: 0.9727 - val_loss: 0.0763\n"
     ]
    }
   ],
   "source": [
    "# Train the model with a higher number of epochs\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_generator),\n",
    "    epochs=15,  # I suggest 15 epochs for better convergence\n",
    "    callbacks=[\n",
    "        # You can use EarlyStopping to prevent overfitting and save computation time\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "        # You can also save the best model weights during training\n",
    "        tf.keras.callbacks.ModelCheckpoint('VDM_v2.keras', monitor='val_loss', save_best_only=True)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "481b9c5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T19:59:43.053306Z",
     "iopub.status.busy": "2025-05-09T19:59:43.053007Z",
     "iopub.status.idle": "2025-05-09T19:59:43.562184Z",
     "shell.execute_reply": "2025-05-09T19:59:43.561612Z"
    },
    "papermill": {
     "duration": 0.784476,
     "end_time": "2025-05-09T19:59:43.563472",
     "exception": false,
     "start_time": "2025-05-09T19:59:42.778996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('violence_detection_model_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0548b71c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T19:59:44.170438Z",
     "iopub.status.busy": "2025-05-09T19:59:44.170155Z",
     "iopub.status.idle": "2025-05-09T19:59:53.244987Z",
     "shell.execute_reply": "2025-05-09T19:59:53.244246Z"
    },
    "papermill": {
     "duration": 9.405377,
     "end_time": "2025-05-09T19:59:53.246124",
     "exception": false,
     "start_time": "2025-05-09T19:59:43.840747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step\n",
      "Prediction: Non-Violent (confidence: 0.01)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def extract_frames_from_video(video_path, num_frames=16, target_size=(224, 224)):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if total_frames < num_frames:\n",
    "        frame_indices = np.linspace(0, total_frames - 1, total_frames).astype(int)\n",
    "    else:\n",
    "        frame_indices = np.linspace(0, total_frames - 1, num_frames).astype(int)\n",
    "\n",
    "    count = 0\n",
    "    grabbed = 0\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        if count in frame_indices:\n",
    "            frame = cv2.resize(frame, target_size)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "            grabbed += 1\n",
    "            if grabbed == len(frame_indices):\n",
    "                break\n",
    "        count += 1\n",
    "    cap.release()\n",
    "    \n",
    "    return np.array(frames)\n",
    "\n",
    "video_path = \"/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence/NV_126.mp4\"\n",
    "frames = extract_frames_from_video(video_path, num_frames=16)\n",
    "frames = frames / 255.0\n",
    "\n",
    "# Predict on each frame, then average\n",
    "preds = model.predict(frames)\n",
    "avg_pred = np.mean(preds)\n",
    "\n",
    "label = \"Violent\" if avg_pred > 0.5 else \"Non-Violent\"\n",
    "print(f\"Prediction: {label} (confidence: {avg_pred:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e1968e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T19:59:53.787384Z",
     "iopub.status.busy": "2025-05-09T19:59:53.787118Z",
     "iopub.status.idle": "2025-05-09T20:00:27.958867Z",
     "shell.execute_reply": "2025-05-09T20:00:27.957882Z"
    },
    "papermill": {
     "duration": 34.442157,
     "end_time": "2025-05-09T20:00:27.960139",
     "exception": false,
     "start_time": "2025-05-09T19:59:53.517982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 100 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:34<00:00,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-Violent       0.96      0.96      0.96        50\n",
      "     Violent       0.96      0.96      0.96        50\n",
      "\n",
      "    accuracy                           0.96       100\n",
      "   macro avg       0.96      0.96      0.96       100\n",
      "weighted avg       0.96      0.96      0.96       100\n",
      "\n",
      "Confusion Matrix:\n",
      "[[48  2]\n",
      " [ 2 48]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Folder paths\n",
    "non_violent_folder = \"/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence\"\n",
    "violent_folder = \"/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence\"\n",
    "\n",
    "# Limit to 100 total videos (50 from each category)\n",
    "non_violent_videos = sorted([os.path.join(non_violent_folder, f) for f in os.listdir(non_violent_folder) if f.endswith(\".mp4\")])[:50]\n",
    "violent_videos = sorted([os.path.join(violent_folder, f) for f in os.listdir(violent_folder) if f.endswith(\".mp4\")])[:50]\n",
    "\n",
    "video_paths = [(path, 0) for path in non_violent_videos] + [(path, 1) for path in violent_videos]\n",
    "\n",
    "def extract_frames(video_path, num_frames=5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    step = max(1, total_frames // num_frames)\n",
    "    \n",
    "    frames = []\n",
    "    for i in range(0, total_frames, step):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.resize(frame, (224, 224))\n",
    "            frames.append(frame)\n",
    "        if len(frames) >= num_frames:\n",
    "            break\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def predict_video(video_path, model):\n",
    "    frames = extract_frames(video_path)\n",
    "    frames = frames.astype('float32') / 255.0\n",
    "    preds = model.predict(frames, verbose=0)\n",
    "    avg_pred = np.mean(preds)\n",
    "    return 1 if avg_pred > 0.5 else 0\n",
    "\n",
    "# Evaluate\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "print(\"Evaluating on 100 videos...\")\n",
    "for path, label in tqdm(video_paths):\n",
    "    pred = predict_video(path, model)\n",
    "    y_true.append(label)\n",
    "    y_pred.append(pred)\n",
    "\n",
    "# Metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Non-Violent\", \"Violent\"]))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0644a800",
   "metadata": {
    "papermill": {
     "duration": 0.338529,
     "end_time": "2025-05-09T20:00:28.585311",
     "exception": false,
     "start_time": "2025-05-09T20:00:28.246782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 176381,
     "sourceId": 397693,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4080.210256,
   "end_time": "2025-05-09T20:00:31.869321",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-09T18:52:31.659065",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
